<!DOCTYPE html>
<!--  This site was created in Webflow. http://www.webflow.com  -->
<!--  Last Published: Sun Jun 14 2020 14:52:00 GMT+0000 (Coordinated Universal Time)  -->
<html data-wf-page="5ee639769e7c204d86365969" data-wf-site="5ee63976202f0569b1ec6f68">
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-135622274-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-135622274-1');
</script>
  
  <!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-135622274-1', 'auto');
ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

  <meta charset="utf-8">
  <title>All about Naive Bayes</title>
  <meta content="All about Naive Bayes" property="og:title">
  <meta content="All about Naive Bayes" property="twitter:title">
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="All about Naive Bayes Understand which Google Cloud tools matches best for you." name="generator">
  <link href="../css/normalize.css" rel="stylesheet" type="text/css">
  <link href="../css/webflow.css" rel="stylesheet" type="text/css">
  <link href="../css/gc-starter.webflow.css" rel="stylesheet" type="text/css">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic"]  }});</script>
  <!-- [if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif] -->
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="../images/GC-logo.ico" rel="shortcut icon" type="image/x-icon">
  <link href="../images/GC-logo.png" rel="apple-touch-icon">
</head>
<body>
  <div data-collapse="medium" data-animation="default" data-duration="400" role="banner" class="navigation w-nav">
    <div class="navigation-items"><a href="../index.html" target="_blank" aria-current="page" class="logo-link w-nav-brand w--current"><img src="../images/logo-gaurav.png" width="104" alt="" class="logo-image"></a>
      <div class="navigation-wrap">
        <nav role="navigation" class="navigation-items w-nav-menu"><a href="../index.html" target="_blank" ria-current="page" class="navigation-item w-nav-link w--current">Home</a>
          
          <a href="../about.html" target="_blank" class="navigation-item w-nav-link">About</a>
          <a href="../blog.html" target="_blank" class="navigation-item w-nav-link">Blogs</a>
          <a href="../contact.html" target="_blank" class="navigation-item w-nav-link">Contact</a></nav>
        <div class="menu-button w-nav-button"><img src="../images/menu-icon_1menu-icon.png" width="22" alt="" class="menu-icon"></div>
      </div>
    </div>
  </div>
  <div class="section">
    <div class="container">
      <div class="w-layout-grid project-overview-grid">
        <div id="w-node-f2c6de040bbf-86365969">
          <h1 class="heading-jumbo">All about Naive Bayes</h1>
          <div class="paragraph-light">All in one place!</div>

        </div>
        <div id="w-node-f2c6de040bc4-86365969">
           
          <div class="paragraph-dark cc-position-name"> 
              <blockquote>
            Learning a Naive Bayes classifier is just a matter of counting how many times each attribute co-occurs with each class
        </blockquote>
     </div>
        </div>
        <div id="w-node-f2c6de040bc9-86365969">
          <div class="paragraph-dark cc-position-name">
            Naive Bayes is the most simple algorithm that you can apply to your data. As the name suggests, here this algorithm makes an assumption as all the variables in the dataset is “Naive” i.e not correlated to each other.
            Naive Bayes is a very popular classification algorithm that is mostly used to get the base accuracy of the dataset.
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="section"><div class="container"><img src="../images/article/Naive Bayes/Naive_bayes.gif" alt="" class="detail-header-image"></div></div>
  
  <div class="section">
    <div class="container">
    <div class="styleguide-block">
      <div class="rich-text w-richtext">
        <h2><u>Explain like I am five</u></h2>
        <br>

        <div class="paragraph-dark">
            Let’s assume that you are walking on the playground. Now you see some red object in front of you. This red object can be a bat or a cat or a ball. You will definitely assume that it will be a ball. But why so?
            <br>
            <br>
            Let’s us think you are making a machine and you have given the task as above to classify an object in between bat, ball and a cat. At first you will think of creating a machine that will identify the characters of the object and then map it with your classification objects such that if an object is a circle then it will be a ball or if the object is living-being then it will be a cat or in our case, if our object is red then it is most probable that it will be a ball.

            <br>
            <br>
            Why so? because from our childhood we have seen a red ball but a red cat or a red bat is very unlikely to our eyes.
            <br>
            <br>
            So in our case, we can classify an object by mapping its features with our classifier individually. As in our case, this red color was mapped with a bat, a cat, and a ball, but eventually, we get the most probability of red object with a ball and therefore we classified that object with a ball.

        </div>
        <br>


        <div> <img src="../images/article/Naive Bayes/NB_eli5.jpeg" alt=""> </div>
        <hr>
        
      </div>
    </div>
    </div>

  </div>




  <div class="section">
    <div class="container">
    <div class="styleguide-block">
      <div class="rich-text w-richtext">
        <h2><u>Formula</u></h2>
        <br>

        <div class="paragraph-dark">
            Here c represents the class eg. ball, cat, bat.
            <br>
            <br>
            x represents features calculated individually.

        </div>
        <br>


        <div> <img src="../images/article/Naive Bayes/formula.png" alt=""> </div>

        <div class="paragraph-dark">
            Where,
            <ul>
                <li>P(c|x) is the posterior probability of class c given predictor ( features).</li>
                <li>P(c) is the probability of class.</li>
                <li>P(x|c) is the likelihood which is the probability of predictor given class.</li>
                <li>P(x) is the prior probability of predictor.</li>
            </ul>

        </div>
        <hr>
        
      </div>
    </div>
    </div>

  </div>



  <div class="section">
    <div class="container">
    <div class="styleguide-block">
      <div class="rich-text w-richtext">
        <h2><u>Example</u></h2>
        <br>

        <div class="paragraph-dark">
            let’s say we have data on 1000 pieces of fruit. The fruit being a Banana, Orange or some other fruit and imagine we know 3 features of each fruit, whether it’s long or not, sweet or not and yellow or not, as displayed in the table below.

        </div>
        <br>


        <div> <img src="../images/article/Naive Bayes/example.png" alt=""> </div>
        <br>
        

        <div class="paragraph-dark">

            So from the table what do we already know?
            <ul>
                <li>50% of the fruits are bananas</li>
                <li>30% are oranges</li>
                <li>20% are other fruits</li>
                
            </ul>
            
            Based on our training set we can also say the following:

            <ul>
                <li>From 500 bananas 400 (0.8) are Long, 350 (0.7) are Sweet and 450 (0.9) are Yellow</li>
                <li>Out of 300 oranges, 0 are Long, 150 (0.5) are Sweet and 300 (1) are Yellow</li>
                <li>From the remaining 200 fruits, 100 (0.5) are Long, 150 (0.75) are Sweet and 50 (0.25) are Yellow</li>
                
            </ul>

            <br>
            Which should provide enough evidence to predict the class of another fruit as it’s introduced.
            <br>
            <br>
            So let’s say we’re given the features of a piece of fruit and we need to predict the class. If we’re told that the additional fruit is Long, Sweet and Yellow, we can classify it using the following formula and subbing in the values for each outcome, whether it’s a Banana, an Orange or Other Fruit. The one with the highest probability (score) being the winner.

        </div>
        <div> <img src="../images/article/Naive Bayes/formula_NB.png" alt=""> </div>
        <br>


        <h4>Banana:</h4>
        <div> <img src="../images/article/Naive Bayes/banana1.png" alt=""> </div>
        <div> <img src="../images/article/Naive Bayes/banana2.png" alt=""> </div>
        <div> <img src="../images/article/Naive Bayes/banana3.png" alt=""> </div>

        <br>
        <h4>Orange:</h4>
        <div> <img src="../images/article/Naive Bayes/orange.png" alt=""> </div>

        <br>
        <h4>Other Fruit:</h4>
        <div> <img src="../images/article/Naive Bayes/other1.png" alt=""> </div>
        <div> <img src="../images/article/Naive Bayes/other2.png" alt=""> </div>
        <div> <img src="../images/article/Naive Bayes/other3.png" alt=""> </div>
        <br>
        <br>

        In this case, based on the higher score ( 0.252 for banana ) we can assume this Long, Sweet and Yellow fruit is in fact, a Banana.
        <hr>

        
      </div>
    </div>
    </div>

  </div>




  <div class="section">
    <div class="container">
    <div class="styleguide-block">
      <div class="rich-text w-richtext">
        <h2><u>Implementation in python</u></h2>
        <br>

        <div class="paragraph-dark">
            Implementation of Naive Bayes algorithm from scratch in python with explanation in each step is uploaded to my Github repository.
            <br>
            <br>
            Implementation of Naive Bayes with help of Scikit learn is also added to my <a href="https://github.com/2796gaurav/Naive-bayes-explained" target="_blank">Github repository.</a>

        </div>
        <hr>
        
      </div>
    </div>
    </div>

  </div>

  <div class="section">
    <div class="container">
    <div class="styleguide-block">
      <div class="rich-text w-richtext">
        <h2><u>Advantages</u></h2>
        <br>

        <div class="paragraph-dark">
            <ul>
                <li>It is easy and fast to predict the class of the test data set. It also performs well in multi-class prediction.</li>
                <li>When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.</li>
                <li>It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).</li>
                
            </ul>
        </div>
        <hr>
        
      </div>
    </div>
    </div>

  </div>


  <div class="section">
    <div class="container">
    <div class="styleguide-block">
      <div class="rich-text w-richtext">
        <h2><u>Disadvantages</u></h2>
        <br>

        <div class="paragraph-dark">
            <ul>
                <li>If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as Zero Frequency. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.</li>
                <li>On the other side naive Bayes is also known as a bad estimator, so the probability outputs are not to be taken too seriously.</li>
                <li>Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.</li>
                
            </ul>
        </div>
        <hr>
        
      </div>
    </div>
    </div>

  </div>


  <div class="section">
    <div class="container">
    <div class="styleguide-block">
      <div class="rich-text w-richtext">
        <h2><u>Applications</u></h2>
        <br>

        <div class="paragraph-dark">
            <ul>
                <li>Real time Prediction: Naive Bayes is an eager learning classifier and it is sure fast. Thus, it could be used for making predictions in real time.</li>
                <li>Multi class Prediction: This algorithm is also well known for multi class prediction feature. Here we can predict the probability of multiple classes of target variable.</li>
                <li>Text classification/ Spam Filtering/ Sentiment Analysis: Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)</li>
                <li>Recommendation System: Naive Bayes Classifier and Collaborative Filtering together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not.</li>
                
            </ul>
        </div>

        <h3>When to use</h3>

        <div class="paragraph-dark">
            <ul>
                <li>Text Classification</li>
                <li>when dataset is huge</li>
                <li>When you have small training set</li>
                
                
            </ul>

            <br>
            Further i will add other machine learning algorithms. The main motto of this article was to give an in depth knowledge of Naive Bayes without using any hard word and explain it from scratch. Further if you want to implement Naive bayes, start from these datasets and you can comment your predicted score with code in the comments section.

            <ul>
                <li>Iris dataset</li>
                <li>Wine dataset</li>
                <li>Adult dataset</li>
            </ul>
        </div>
        <hr>
        
      </div>
    </div>
    </div>

  </div>




  <div class="section">
    <div class="container">
      <div class="email-section">
        <h3>Want to work together?</h3>
        <p>If you like what you see and want to work together, get in touch!</p><a href="mailto:gauravc2708@gmail.com?subject=You&#x27;ve%20got%20mail!" class="email-link">gauravc2708@gmail.com</a></div>
    </div>
  </div>
  <div class="footer-wrap">

    <div class="footer-links"><a href="https://www.linkedin.com/in/gauravc2708/" target="_blank" class="footer-item">Linkedin</a><a href="https://twitter.com/2796gaurav" target="_blank" class="footer-item">Twitter</a><a href="https://medium.com/@gauravc2708" target="_blank" class="footer-item">Medium</a></div>
  </div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.4.1.min.220afd743d.js?site=5ee63976202f0569b1ec6f68" type="text/javascript" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
  <script src="../js/webflow.js" type="text/javascript"></script>
  <!-- [if lte IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif] -->
</body>
</html>